# Pipeline de Dados de Produtos com Airflow e Metabase

Este reposit√≥rio cont√©m um projeto completo de engenharia de dados que implementa um pipeline ETL (Extra√ß√£o, Transforma√ß√£o e Carga) para an√°lise de dados de vendas de produtos. A solu√ß√£o √© totalmente containerizada usando Docker e orquestrada com Apache Airflow, seguindo as melhores pr√°ticas como a arquitetura Medallion e o provisionamento de infraestrutura como c√≥digo.

## Vis√£o Geral

O objetivo deste projeto √© extrair dados de vendas de uma API, process√°-los atrav√©s de um pipeline robusto, armazen√°-los em um banco de dados PostgreSQL e, finalmente, disponibiliz√°-los para an√°lise e visualiza√ß√£o em uma ferramenta de Business Intelligence (Metabase).

## Principais Funcionalidades

*   **Orquestra√ß√£o com Apache Airflow:** Pipeline de dados agendado e monitorado atrav√©s de uma DAG (Directed Acyclic Graph) do Airflow.
*   **Arquitetura Medallion:** Os dados s√£o processados em tr√™s camadas distintas (`Bronze`, `Silver` e `Gold`) para garantir qualidade, rastreabilidade e governan√ßa.
*   **Enriquecimento de Dados Geogr√°ficos:** O pipeline n√£o apenas limpa, mas tamb√©m enriquece os dados com informa√ß√µes geogr√°ficas, como regi√µes do Brasil e c√°lculo de dist√¢ncias, al√©m de tratar coordenadas inv√°lidas.
*   **Infraestrutura como C√≥digo:** Todo o ambiente, incluindo o banco de dados e a ferramenta de BI, √© definido e gerenciado pelo arquivo `docker-compose.override.yml`.
*   **Data Marts e KPIs:** A camada `Gold` cont√©m tabelas agregadas (Data Marts) e KPIs (Key Performance Indicators) prontos para o consumo por analistas de dados e dashboards.
*   **Visualiza√ß√£o com Metabase:** O projeto inclui um servi√ßo do Metabase pr√©-configurado para se conectar facilmente ao banco de dados e explorar os resultados.

## Arquitetura da Solu√ß√£o

O fluxo de dados segue as etapas abaixo:

```mermaid
graph TD
    A[API de Produtos] --> B{Apache Airflow};
    B --> C[PostgreSQL];
    C --> D{Metabase};

    subgraph B [Pipeline ETL]
        direction LR
        T1(1. Extrair para Bronze) --> T2(2. Transformar para Silver) --> T3(3. Carregar para Gold);
    end

    subgraph C [Banco de Dados]
        direction TB
        L1[Bronze: Dados Brutos] --> L2[Silver: Dados Limpos e Enriquecidos] --> L3[Gold: Marts e KPIs];
    end

    subgraph D [Business Intelligence]
        direction TB
        V1[Dashboards] --> V2[An√°lises];
    end
```

1.  **Extra√ß√£o (Bronze):** A DAG do Airflow (`pipeline_produtos_v3`) √© executada diariamente, extraindo os dados da API `https://labdados.com/produtos` e salvando-os em formato JSON bruto na tabela `bronze.raw_produtos`.
2.  **Transforma√ß√£o (Silver):** Os dados brutos s√£o limpos, padronizados, e enriquecidos. Coordenadas geogr√°ficas inv√°lidas s√£o corrigidas, e novas colunas (como `regiao` e `distancia_sp_km`) s√£o adicionadas. O resultado √© salvo na tabela `silver.produtos_clean`.
3.  **Carga (Gold):** A partir dos dados limpos, s√£o criadas diversas tabelas anal√≠ticas (marts) e KPIs na camada `Gold`. Essas tabelas s√£o agregadas e otimizadas para consultas de BI.
4.  **Visualiza√ß√£o:** O Metabase se conecta √† camada `Gold` do PostgreSQL, permitindo que usu√°rios de neg√≥cio criem dashboards e an√°lises de forma intuitiva.

## üõ†Ô∏è Stack de Tecnologias

*   **Orquestra√ß√£o:** Apache Airflow
*   **Processamento de Dados:** Python, Pandas
*   **Banco de Dados:** PostgreSQL
*   **BI & Visualiza√ß√£o:** Metabase
*   **Containeriza√ß√£o:** Docker, Docker Compose

## üöÄ Como Executar o Projeto

### Pr√©-requisitos

*   [Docker](https://www.docker.com/get-started)
*   [Docker Compose](https://docs.docker.com/compose/install/) (geralmente inclu√≠do na instala√ß√£o do Docker Desktop)
*   [Astro CLI](https://www.astronomer.io/docs/astro/cli/install-cli) para um ambiente de desenvolvimento Airflow local simplificado.

### 1. Configura√ß√£o da Infraestrutura

O `docker-compose.override.yml` provisiona os servi√ßos de banco de dados e Metabase.

Para iniciar a infraestrutura, execute o seguinte comando na raiz do projeto:

```bash
astro dev start
```

Isso ir√°:
*   Criar um container PostgreSQL chamado `astro_postgres_extra` acess√≠vel na porta `5433` da sua m√°quina local.
*   Executar o script `init-db.sql` para criar os bancos de dados (`analytics`, `metabase_app`), os schemas (`bronze`, `silver`, `gold`) e o usu√°rio para o Airflow.
*   Criar um container Metabase chamado `astro_metabase` acess√≠vel em [http://localhost:3000](http://localhost:3000).

### 2. Configura√ß√£o do Airflow

Para que a DAG possa se conectar ao banco de dados, voc√™ precisa configurar uma conex√£o no Airflow.

*   **ID da Conex√£o (Conn Id):** `postgres_analytics`
*   **Tipo da Conex√£o (Conn Type):** `Postgres`
*   **Host:** `postgres-extra` (nome do servi√ßo no Docker Compose)
*   **Schema:** `analytics`
*   **Login:** `airflow_user`
*   **Senha:** `airflow_pass`
*   **Porta:** `5433`

Ap√≥s configurar a conex√£o, ative a DAG `pipeline_produtos_v3` na UI do Airflow e dispare uma execu√ß√£o manual para popular o banco de dados.

### 3. Configura√ß√£o do Metabase

1.  Acesse o Metabase em [http://localhost:3000](http://localhost:3000).
2.  Siga as instru√ß√µes iniciais para criar uma conta de administrador.
3.  Na etapa de adicionar dados, selecione "PostgreSQL" e preencha com as seguintes informa√ß√µes:
    *   **Host:** `postgres-extra` (nome do servi√ßo no Docker Compose)
    *   **Porta:** `5432` (porta interna da rede Docker)
    *   **Nome do banco de dados:** `analytics`
    *   **Nome de usu√°rio:** `postgres`
    *   **Senha:** `postgres`
4.  Pronto! Agora voc√™ pode explorar as tabelas do schema `gold` e come√ßar a criar suas an√°lises e dashboards.

## Detalhes do Pipeline (`pipeline_produtos_v3`)

A DAG √© o cora√ß√£o do projeto e √© composta pelas seguintes tarefas principais:

*   **`criar_schemas` e `criar_tabelas`:** Garantem que a infraestrutura no banco de dados (schemas e tabelas da camada Bronze) exista antes da execu√ß√£o.
*   **`extrair_dados_api`:** Conecta-se √† API, extrai os dados e os armazena na tabela `bronze.raw_produtos`.
*   **`transformar_para_silver`:** L√™ os dados da camada Bronze e aplica uma s√©rie de transforma√ß√µes:
    *   Normaliza a estrutura JSON.
    *   Converte tipos de dados (`Data da Compra` para datetime, colunas num√©ricas).
    *   Padroniza campos de texto.
    *   Valida e corrige coordenadas geogr√°ficas, usando o centro do estado como fallback.
    *   Enriquece os dados com a `regiao` e a `distancia_sp_km`.
*   **`criar_marts_gold`:** Cria agrega√ß√µes de neg√≥cio, como:
    *   `mart_vendas_categoria`: Vendas, pre√ßo m√©dio e avalia√ß√£o por categoria.
    *   `mart_performance_estado`: Vendas, frete e avalia√ß√£o por estado.
    *   `mart_vendas_temporal`: An√°lise de vendas ao longo do tempo.
    *   `mart_top_produtos`: Ranking dos produtos mais vendidos.
*   **`criar_mart_geografico`:** Cria uma tabela otimizada para visualiza√ß√µes em mapas, com coordenadas m√©dias por estado e rankings.
*   **`criar_kpis_dashboard`:** Gera tabelas com KPIs consolidados para alimentar dashboards de alta performance, incluindo m√©tricas de crescimento e participa√ß√£o de mercado.
*   **`log_pipeline_stats`:** Ao final, registra um resumo da execu√ß√£o, informando quantos registros foram processados em cada etapa.

## Autor
[Wellington M Santos](https://www.linkedin.com/in/wellington-moreira-santos/)
